# -*- coding: utf-8 -*-
"""Hugging-Face Dataset Builder

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15Vbba3uEb25JGsv5dJfUFYNaxZxvWznK
"""

# coding=utf-8
# Copyright 2020 HuggingFace Datasets Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


import collections

import datasets


_DESCRIPTION = """\
Preprocessed Dataset from IWSLT'15 English-Vietnamese machine translation: English-Vietnamese.
"""

_CITATION = """\
@inproceedings{Luong-Manning:iwslt15,
        Address = {Da Nang, Vietnam}
        Author = {Luong, Minh-Thang  and Manning, Christopher D.},
        Booktitle = {International Workshop on Spoken Language Translation},
        Title = {Stanford Neural Machine Translation Systems for Spoken Language Domain},
        Year = {2015}}
"""

_TRAIN_URL = "https://github.com/TENZINKHORLO20/Machine_Translation/raw/main/Datasets/train.zip"
_TEST_URL = "https://github.com/TENZINKHORLO20/Machine_Translation/raw/main/Datasets/test.zip"
_VALIDATION_URL = "https://github.com/TENZINKHORLO20/Machine_Translation/raw/main/Datasets/validation.zip"

# Tuple that describes a single pair of files with matching translations.
# language_to_file is the map from language (2 letter string: example 'en')
# to the file path in the extracted directory.
TranslateData = collections.namedtuple("TranslateData", ["url", "language_to_file"])


class MT_En_FrConfig(datasets.BuilderConfig):
    """BuilderConfig for MT_Eng_Vietnamese."""

    def __init__(self, language_pair=(None, None), **kwargs):
        """BuilderConfig for MT_Eng_Vi.
        Args:
            for the `datasets.features.text.TextEncoder` used for the features feature.
          language_pair: pair of languages that will be used for translation. Should
            contain 2-letter coded strings. First will be used at source and second
            as target in supervised mode. For example: ("vi", "en").
          **kwargs: keyword arguments forwarded to super.
        """

        description = ("Translation dataset from %s to %s") % (language_pair[0], language_pair[1])
        super(MT_En_FrConfig, self).__init__(
            description=description,
            version=datasets.Version("1.0.0"),
            **kwargs,
        )
        self.language_pair = language_pair


class MT_En_Fr(datasets.GeneratorBasedBuilder):
    """English Vietnamese machine translation dataset from IWSLT2015."""

    BUILDER_CONFIGS = [
        MT_En_FrConfig(
            name="en-fr",
            language_pair=("en", "fr"),
        ),
    ]
    BUILDER_CONFIG_CLASS = MT_En_FrConfig

    def _info(self):
        source, target = self.config.language_pair
        return datasets.DatasetInfo(
            description=_DESCRIPTION,
            features=datasets.Features(
                {"translation": datasets.features.Translation(languages=self.config.language_pair)}
            ),
            supervised_keys=(source, target),
            homepage="https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/",
            citation=_CITATION,
        )

    def _split_generators(self, dl_manager):
        source, target = self.config.language_pair

        # path = dl_manager.download_and_extract(self._DATA_URL)

        # return [
        #     datasets.SplitGenerator(
        #         name=datasets.Split.TRAIN, gen_kwargs={"filepath": path+"/train_data_one.jsonl"}
        #     ),
        # ]

        files = {}
        for split in ("train", "validation", "test"):
            if split == "test":
                dl_dir = dl_manager.download_and_extract(_TEST_URL)
            if split == "validation":
                dl_dir = dl_manager.download_and_extract(_VALIDATION_URL)
            if split == "train":
                dl_dir = dl_manager.download_and_extract(_TRAIN_URL)

            files[split] = {"source_file": dl_dir+'/{}_en.txt'.format(split), "target_file": dl_dir+'/{}_fr.txt'.format(split)}

        return [
            datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs=files["train"]),
            datasets.SplitGenerator(name=datasets.Split.VALIDATION, gen_kwargs=files["validation"]),
            datasets.SplitGenerator(name=datasets.Split.TEST, gen_kwargs=files["test"]),
        ]

    def _generate_examples(self,source_file,target_file):
        source, target = self.config.language_pair

        """This function returns the examples in the raw (text) form."""
        with open(source_file, encoding="utf-8") as f:
            source_sentences = f.read().split("\n")
            source_sentences.remove("")
        f.close()
        with open(target_file, encoding="utf-8") as f:
            target_sentences = f.read().split("\n")
            target_sentences.remove("")
        f.close()

        for idx, (l1, l2) in enumerate(zip(source_sentences, target_sentences)):
            result = {"translation": {source: l1, target: l2}}
            # Make sure that both translations are non-empty.
            yield idx, result